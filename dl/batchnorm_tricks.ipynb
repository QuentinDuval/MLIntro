{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Synching batch norm\n",
    "---\n",
    "\n",
    "Do not forget to synchronize batch normalization across nodes in distributed training, or the network might learn something specific to your specific shard of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbn = nn.SyncBatchNorm(num_features=32) # synchronized automatically\n",
    "bn = nn.BatchNorm2d(num_features=32) # not synchronized\n",
    "\n",
    "# Automatic transformation (can be applied recursively to a network)\n",
    "nn.SyncBatchNorm.convert_sync_batchnorm(bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "\n",
    "* https://pytorch.org/docs/master/generated/torch.nn.SyncBatchNorm.html\n",
    "* https://pytorch.org/docs/master/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Calling eval()\n",
    "---\n",
    "\n",
    "Do not forget to call eval() during inference time, otherwise the batch normalization module will still adapt to the data, which is not something you likely wish outside training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0067,  0.0124])\n",
      "tensor([ 0.0052, -0.0009])\n",
      "tensor([ 0.0072, -0.0063])\n",
      "tensor([ 0.0072, -0.0063])\n",
      "tensor([ 0.0072, -0.0063])\n",
      "tensor([ 0.0072, -0.0063])\n"
     ]
    }
   ],
   "source": [
    "bn = nn.BatchNorm1d(num_features=2)\n",
    "x = torch.zeros(size=(100, 2))\n",
    "\n",
    "# Moving values\n",
    "bn.train()\n",
    "for _ in range(3):\n",
    "    x.normal_()\n",
    "    bn(x)\n",
    "    print(bn.running_mean)\n",
    "\n",
    "# Frozen values\n",
    "bn.eval()\n",
    "for _ in range(2):\n",
    "    x.normal_()\n",
    "    bn(x)\n",
    "    print(bn.running_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
