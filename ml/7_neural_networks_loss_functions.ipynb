{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# MSE Loss\n",
    "---\n",
    "\n",
    "For regression, corresponds to the assumption that the data generating process follows a gaussian probability distribution around the value to be found (basically, assumes that the noise is gaussian, and that the output is mono-modal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99\n",
      "1.99\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "x = torch.zeros(size=(10, 3, 32, 32))\n",
    "y = torch.zeros(size=(10, 3, 32, 32))\n",
    "x.normal_()\n",
    "y.normal_()\n",
    "\n",
    "mse = nn.MSELoss(reduction='mean')\n",
    "print(f\"{mse(x, y).item():.2f}\")\n",
    "print(f\"{mse(y, x).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Cross Entropy Loss\n",
    "---\n",
    "\n",
    "In classification problems, the goal is to maximize the joint probability of guessing the right class for a list of samples $(x_i, y_i)$. Maximizing this probability, assuming i.i.d. samples, takes the form of maximizing the product:\n",
    "\n",
    "&emsp; $\\displaystyle P = \\prod_i p(y_i|x_i) \\implies \\log P = \\sum_i \\log p(y_i|x_i)$\n",
    "\n",
    "This is equivalent to minimizing the loss function, which **negative log likelihood**:\n",
    "\n",
    "&emsp; $\\displaystyle \\mathcal{L} = - \\sum_i \\log p(y_i|x_i)$\n",
    "\n",
    "Since in most networks, the outputs of the network are not bounded, we generally apply a **softmax** function to transform the **logits** of each class to probabilities:\n",
    "\n",
    "&emsp; $\\displaystyle p(y_i|x_i) = \\frac{\\exp(l_i)}{\\sum_j \\exp(l_j)}$ where $l_i$ is the logit output for class *i*\n",
    "\n",
    "This softmax function correspond to the assumption that the classes are linearly separable in their final representation (the previous layers of the network create this representation) which itself correspond to the assumption that the N classes are centered around N point, following a gaussian process centered on that point, with the same variance for all classes.\n",
    "\n",
    "The *CrossEntropyLoss* class of Pytorch combines both the application of Softmax and the Negative Log Likelihood in one class (but is more stable numerically):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6094)\n",
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor(1.6094)\n",
      "tensor(1.6094)\n"
     ]
    }
   ],
   "source": [
    "# Example for 5 classes, and a batch size of 10\n",
    "\n",
    "logits = torch.zeros(size=(10, 5))\n",
    "target = torch.LongTensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "print(ce(logits, target))\n",
    "\n",
    "# Equivalent through Softmax\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probs = softmax(logits)\n",
    "print(probs) # each class has same probability by construction here\n",
    "nnl = nn.NLLLoss()\n",
    "print(nnl(torch.log(probs), target))\n",
    "\n",
    "# Equivalent through LogSoftmax\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "nnl = nn.NLLLoss()\n",
    "print(nnl(log_softmax(logits), target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Hinge loss\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Contrastive Loss\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
