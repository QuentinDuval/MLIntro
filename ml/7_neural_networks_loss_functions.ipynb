{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# MSE Loss\n",
    "---\n",
    "\n",
    "For regression, corresponds to the assumption that the data generating process follows a gaussian probability distribution around the value to be found (basically, assumes that the noise is gaussian, and that the output is mono-modal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99\n",
      "1.99\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "x = torch.zeros(size=(10, 3, 32, 32))\n",
    "y = torch.zeros(size=(10, 3, 32, 32))\n",
    "x.normal_()\n",
    "y.normal_()\n",
    "\n",
    "mse = nn.MSELoss(reduction='mean')\n",
    "print(f\"{mse(x, y).item():.2f}\")\n",
    "print(f\"{mse(y, x).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Cross Entropy Loss\n",
    "---\n",
    "\n",
    "In classification problems, the goal is to maximize the joint probability of guessing the right class for a list of samples $(x_i, y_i)$. Maximizing this probability, assuming i.i.d. samples, takes the form of maximizing the product:\n",
    "\n",
    "&emsp; $\\displaystyle P = \\prod_i p(y_i|x_i) \\implies \\log P = \\sum_i \\log p(y_i|x_i)$\n",
    "\n",
    "This is equivalent to minimizing the loss function, which **negative log likelihood**:\n",
    "\n",
    "&emsp; $\\displaystyle \\mathcal{L} = - \\sum_i \\log p(y_i|x_i)$\n",
    "\n",
    "Since in most networks, the outputs of the network are not bounded, we generally apply a **softmax** function to transform the **logits** of each class to probabilities:\n",
    "\n",
    "&emsp; $\\displaystyle p(y_i|x_i) = \\frac{\\exp(l_i)}{\\sum_j \\exp(l_j)}$ where $l_i$ is the logit output for class *i*\n",
    "\n",
    "This softmax function correspond to the assumption that the classes are linearly separable in their final representation (the previous layers of the network create this representation) which itself correspond to the assumption that the N classes are centered around N point, following a gaussian process centered on that point, with the same variance for all classes.\n",
    "\n",
    "The *CrossEntropyLoss* class of Pytorch combines both the application of Softmax and the Negative Log Likelihood in one class (but is more stable numerically):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6094)\n",
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]])\n",
      "tensor(1.6094)\n",
      "tensor(1.6094)\n"
     ]
    }
   ],
   "source": [
    "# Example for 5 classes, and a batch size of 10\n",
    "\n",
    "logits = torch.zeros(size=(10, 5))\n",
    "target = torch.LongTensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "print(ce(logits, target))\n",
    "\n",
    "# Equivalent through Softmax\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probs = softmax(logits)\n",
    "print(probs) # each class has same probability by construction here\n",
    "nnl = nn.NLLLoss()\n",
    "print(nnl(torch.log(probs), target))\n",
    "\n",
    "# Equivalent through LogSoftmax\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "nnl = nn.NLLLoss()\n",
    "print(nnl(log_softmax(logits), target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Contrastive Loss\n",
    "---\n",
    "\n",
    "The contrastive loss is used a lot in Self Supervised Learning for images and videos. To make a model invariant to rotations, crops, noise, color jittering and such, a model is trained to recognize that two random augmentations of the same images are identical.\n",
    "\n",
    "The notion of identical is typically measured by the **cosine similarity** between the representation of the two augmentations of the image.\n",
    "\n",
    "&emsp; $\\displaystyle \\mathcal{S}(x_i, x_j) = \\frac{x_i . x_j}{\\Vert x_i \\Vert \\Vert x_j \\Vert}$ (but any form of similarity function $\\mathcal{S}$ can do)\n",
    "\n",
    "The problem with training a neural network to do so, is that a trivial solution is to assign all images to the same representation (constant function) so that indeed, all augmentations are perfectly recognized. The idea of contrastive learning is to avoid those trivial solution by also making sure that two related images have different representations.\n",
    "\n",
    "The network is therefore trained to increase the similary between the representation of two random augmentations of the same image, and to decrease this similarity between two random augmentations of two different images. To do so, we deal with images the same way as we would do with words in a sentence:\n",
    "\n",
    "* each image is thought as having its own label\n",
    "* we try to \"classify\" correctly the representations\n",
    "\n",
    "We therefore use a modified softmax function (good for classifying) together with the similarity function we talked about before, giving the loss between a **positive pair** (two representations $x_i$ and $x_j$ of two augmentations of the same image):\n",
    "\n",
    "&emsp; $\\displaystyle \\mathcal{L}(i,j) = \\frac{\\exp \\mathcal{S}(x_i, x_j)}{\\sum_{k \\ne i} \\exp \\mathcal{S}(x_i, x_k)}$ (classify the $x_j$ as the sole matching element)\n",
    "\n",
    "We usually add an hyper parameter named the **temperature** $\\tau$ to get the full formula:\n",
    "\n",
    "&emsp; $\\displaystyle \\mathcal{L}(i,j) = \\frac{\\exp \\big( \\mathcal{S}(x_i, x_j) / \\tau \\big)}{\\sum_{k \\ne i} \\exp \\big( \\mathcal{S}(x_i, x_k) / \\tau \\big)}$\n",
    "\n",
    "The parameter $\\tau$ controls how much the similarity is enforced to be strong between positive pairs, for we have: $\\exp \\big( s / \\tau \\big) = \\sqrt[\\tau] \\exp s$\n",
    "\n",
    "Note: for reference, check SimCLR: https://arxiv.org/pdf/2002.05709.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Hinge loss\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
