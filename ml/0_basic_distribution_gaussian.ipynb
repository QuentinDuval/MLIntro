{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Univariate Gaussian\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for a univariate Gaussian (or normal) distribution:\n",
    "\n",
    "&emsp; $\\boxed{\\mathcal{N}(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\textstyle - \\frac{1}{2 \\sigma^2} (x-\\mu)^2} = \\frac{\\vert \\beta \\vert}{\\sqrt{2 \\pi}} e^{\\textstyle - \\frac{\\beta}{2}(x-\\mu)^2}}$\n",
    "&emsp; where\n",
    "&emsp; $\\mathbb{E}[x] = \\mu$\n",
    "&emsp; and\n",
    "&emsp; $\\mathbb{V}[x] = \\sigma^2 = \\beta^{-1}$\n",
    "\n",
    "Knowing the mean $\\mu$ and variance $\\sigma^2$ (or alternatively the precisions $\\beta$) of a gaussian distribution is enough to completely characterize the distribution. The mean is also the mode (the highest value of the distribution):\n",
    "\n",
    "* The higher the variance $\\sigma^2$, the smaller the precision $\\beta$, the more spread the distribution is\n",
    "* The higher the precision $\\beta$, the higher the precision $\\sigma^2$, the more centralized the distribution is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Product of gaussians\n",
    "\n",
    "The product of gaussian appears quite often in Bayesian treatment of machine learning, in which we will often marginalize the probability of $x$ over the parameters $w$ of a model $p(x|w)$:\n",
    "\n",
    "&emsp; $\\displaystyle p(x) = \\int_{\\mathcal{W}} p(x|w) \\, p(w) \\, dw$\n",
    "\n",
    "If $w$ follows a gaussian distribution, and if $x$ is gaussian, with a mean that is a linear function on $w$, then the resulting distribution is also gaussian:\n",
    "\n",
    "&emsp; $p(w) = \\mathcal{N}(\\mu_{w}, \\; \\sigma_{w}^2)$\n",
    "&emsp; and\n",
    "&emsp; $p(x|w) = \\mathcal{N}(a w + b, \\; \\alpha^2)$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $p(x) = \\mathcal{N}(a \\mu_w + b, \\; a^2 \\sigma_w^2 + \\alpha^2)$\n",
    "\n",
    "This has a link with correlation, which is the reduction of variance of $x$, knowing $w$.\n",
    "\n",
    "&emsp; $\\displaystyle r^2 = \\frac{\\sigma_x^2 - \\alpha^2}{\\sigma_x^2} = \\frac{a^2 \\sigma_w^2}{\\sigma_x^2}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\displaystyle r = \\frac{a \\sigma_w}{\\sigma_x} = \\frac{a \\sigma_w^2}{\\sigma_x \\sigma_w} = \\frac{\\mathbb{C}\\text{ov}[x,w]}{\\sqrt{\\mathbb{V}[x] \\mathbb{V}[w]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Convolutions of gaussians\n",
    "\n",
    "This previous formula can be applied for the convolution of two gaussian. This occurs whenever we want to know the distribution of the **sum of two independent** variables $t = x + y$ that each follow a gaussian distribution:\n",
    "\n",
    "&emsp; $\\displaystyle p(t) = \\int_{\\mathcal{X}} p(t|x) \\, p(x) \\, dx$\n",
    "&emsp; where\n",
    "&emsp; $p(t|x) = \\mathcal{N}(x + \\mu_y, \\, \\sigma_y^2)$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\displaystyle \\boxed{p(x+y) = \\mathcal{N}(\\mu_x + \\mu_y, \\; \\sigma_x^2 + \\sigma_y^2)}$\n",
    "\n",
    "Note that this formula is also true for any distribution, because of linearity of the expectation:\n",
    "\n",
    "&emsp; $\\mathbb{E}[x+y] = \\mathbb{E}[x] + \\mathbb{E}[y]$\n",
    "&emsp; and\n",
    "&emsp; $\\mathbb{V}[x+y] = \\mathbb{E}[(x+y)^2] = \\mathbb{E}[x^2] + \\mathbb{E}[y^2] + 2 \\mathbb{E}[xy] = \\mathbb{V}[x] + \\mathbb{V}[y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Moments\n",
    "\n",
    "The p-moment of a distribution is defined as the expected value of functions $x \\mapsto x^p$. The central p-moments of a distribution are defined as the expected value of functions $x \\mapsto (x-\\mu)^p$. Because the distribution is symmetric, the central moments are zero for even values of $p$.\n",
    "\n",
    "&emsp; $\\mathbb{E}[x] = \\mu$ (first moment)\n",
    "\n",
    "&emsp; $\\mathbb{E}[x^2] = \\mu^2 + \\sigma^2$ (second moment)\n",
    "\n",
    "&emsp; $\\mathbb{E}[(x-\\mu)^2] = \\sigma^2 = \\mathbb{V}[x]$ (central second moment, the variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exponential distribution\n",
    "\n",
    "The exponential distribution is part of the exponential distributions, of the form:\n",
    "\n",
    "&emsp; $\\displaystyle p(x|\\theta) = h(x) \\, g(\\theta ) \\, \\exp \\big( \\eta (\\theta )\\cdot T(x) \\big)$\n",
    "\n",
    "Where the parameters $\\theta = (\\mu, \\sigma^2)$ and we have the following (not unique) decomposition:\n",
    "\n",
    "&emsp; $T(x) = \\begin{pmatrix} 1 \\\\ x \\\\ x^2 \\end{pmatrix}$\n",
    "&emsp; $\\eta(\\theta) = \\begin{pmatrix} -\\mu^2 / (2 \\sigma^2) \\\\ \\mu / \\sigma^2 \\\\ - 1 / (2 \\sigma^2) \\end{pmatrix}$\n",
    "&emsp; $\\displaystyle h(x) = \\frac{1}{\\sqrt{2 \\pi}}$\n",
    "&emsp; $\\displaystyle g(\\theta) = \\frac{1}{\\sqrt{\\sigma^2}}$\n",
    "\n",
    "So if we have an exponential, with inside it an **order 2 polynomial in x**, we get a Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Indefinite integral\n",
    "\n",
    "The gaussian distribution **does not have an indifinite integral**. We can still compute the square of its integral, and this is how we find the normalizing constant. To evaluate the integral of $e^{-x^2}$, we use a multivariate gaussian with 2 variables:\n",
    "\n",
    "&emsp; $\\displaystyle I = \\int_{-\\infty}^{\\infty} e^{-x^2} dx = 2 \\int_0^{\\infty} e^{-x^2} dx$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\displaystyle I^2 = 4 \\int_{-\\infty}^{\\infty} e^{-x^2} dx \\int_{-\\infty}^{\\infty} e^{-y^2} dy = 4 \\int_0^{\\infty} \\Bigg( \\int_0^{\\infty} e^{-x^2(1 + \\frac{y^2}{x^2})} dy \\Bigg) dx$\n",
    "\n",
    "We then do the change of variables $s = \\frac{y}{x}$ which implies $dy = x \\, ds$:\n",
    "\n",
    "&emsp; $\\displaystyle I^2 = 4 \\int_0^{\\infty} \\Bigg( \\int_0^{\\infty} e^{-x^2(1 + s^2)} x ds \\Bigg) dx = 4 \\int_0^{\\infty} \\Bigg( \\int_0^{\\infty} \\frac{-2 x (1 + s^2)}{- 2(1 + s^2)} e^{-x^2(1 + s^2)} dx \\Bigg) ds$\n",
    "\n",
    "&emsp; $\\displaystyle I^2 = 4 \\int_0^{\\infty} \\Big[ \\frac{1}{- 2(1 + s^2)} e^{-x^2(1 + s^2)} \\Big]_{x=0}^{x=\\infty} ds = 4 \\int_0^{\\infty} \\frac{1}{2(1 + s^2)} ds = 2 \\Big[ \\arctan s \\Big]_{s=0}^{s=\\infty} = \\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Maximizing entropy\n",
    "\n",
    "The Gaussian distribution maximizes the differential entropy **for a given mean and variance**:\n",
    "\n",
    "&emsp; $\\displaystyle h(p) = - \\int_{-\\infty}^{\\infty} p(x) \\log p(x) \\, dx$\n",
    "\n",
    "To prove it, we consider the KL divergence between any other distribution $f$ and the normal distribution $g$ and use the property that the KL divergence is always positive:\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(f||g) = - \\int f(x) \\log \\frac{g(x)}{f(x)} dx = - h(f) - \\int f(x) \\big( \\log \\frac{1}{\\sqrt {2 \\pi \\sigma^2}} - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\big) dx$\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(f||g) = - h(f) + \\frac{1}{2} \\log (2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\int f(x) (x-\\mu)^2 dx$\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(f||g) = - h(f) + \\frac{1}{2} \\log (2 \\pi \\sigma^2) + \\frac{\\sigma^2}{2 \\sigma^2} = -h(f) + h(g) \\ge 0$\n",
    "\n",
    "Where the differential entropy of the gaussian distribution is:\n",
    "\n",
    "&emsp; $\\displaystyle h(p) = - \\int_{-\\infty}^{\\infty} p(x) \\log p(x) \\, dx = \\log (\\sigma \\sqrt{2 \\pi}) + \\frac{1}{2} = \\frac{1}{2} \\log ( 2 \\pi \\sigma^2 ) + \\frac{1}{2}$\n",
    "&emsp; (see [this link](https://proofwiki.org/wiki/Differential_Entropy_of_Gaussian_Distribution))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Multivariate Gaussian\n",
    "---\n",
    "\n",
    "* slicing\n",
    "* not the same as conditional gaussian\n",
    "* laws of deductions\n",
    "* PCA to find the axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Multiple data points\n",
    "---\n",
    "\n",
    "* independence => covariance matrix of the form $\\alpha^{-1} I_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Conjugate priors\n",
    "---\n",
    "\n",
    "* todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
