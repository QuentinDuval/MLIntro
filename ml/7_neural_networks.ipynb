{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of ANN\n",
    "# equations for back-prop\n",
    "# typical error functions (cross entropy, MSE loss)\n",
    "# why optim in high dimension works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Feed forward networks\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Limitations of fixed sets of basis functions\n",
    "\n",
    "* explain we want to learn the best set of functions\n",
    "\n",
    "<br>\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "* Learning the computation of nested functions\n",
    "* Not necessarily perceptron (exemple of CNN) but piecewise linear functions are easier to optimize\n",
    "\n",
    "<br>\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "Let us consider a chain of functions 3 layers deep $(f_1, f_2, f_3)$ with respective parameters $(\\theta_1, \\theta_2, \\theta_3)$ followed by a loss function. The computation of the loss is done with a **forward** pass:\n",
    "\n",
    "&emsp; $x$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $h_1 = f_1(x,\\theta_1)$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $h_2 = f_2(h_1,\\theta_2)$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $h_3 = f_3(h_2,\\theta_3)$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $e = loss(h_3)$\n",
    "\n",
    "To adjust the parameters $(\\theta_1, \\theta_2, \\theta_3)$ of the function, in order to minimize the loss via gradient descent, we have to compute the partial derivative of the loss with respect all parameters:\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial e}{\\partial \\theta_3} = \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial \\theta_3}$\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial e}{\\partial \\theta_2} = \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial h_2} \\times \\frac{\\partial h_2}{\\partial \\theta_2}$\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial e}{\\partial \\theta_1} = \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial h_2} \\times \\frac{\\partial h_2}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial \\theta_1}$\n",
    "\n",
    "We can see that the prefix of the derivative match up, and so the most efficient way to compute the derivatives is by doing a **backward** pass, memoizing the results for the next set of parameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Back-propagation (multi parameters)\n",
    "\n",
    "* Jacobian matrices instead of gradients, but notation can still be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Parameters sharing\n",
    "---\n",
    "\n",
    "* todo: and explain CNN and the implementation with gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Recurrent networks\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Back propagation\n",
    "\n",
    "Let us consider a chain of 3 successive call to the function $f$ (for a sequence of 3 elements):\n",
    "\n",
    "&emsp; $x_0, h_0$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $h_1 = f(x_0, h_0,\\theta)$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $h_2 = f(x_1, h_1,\\theta)$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $h_3 = f(x_2, h_2,\\theta)$\n",
    "&emsp; $\\mapsto$\n",
    "&emsp; $e = loss(h_3)$\n",
    "\n",
    "To adjust the parameters $\\theta$ of the function $f$, in order to minimize the loss via gradient descent, we have to compute the partial derivative of the loss with respect to $\\theta$:\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial e}{\\partial \\theta} = \\frac{\\partial e}{\\partial h_3} \\big( \\frac{\\partial h_3}{\\partial \\theta} + \\frac{\\partial h_3}{\\partial h_2} \\big( \\frac{\\partial h_2}{\\partial \\theta} + \\frac{\\partial h_2}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial \\theta} \\big) \\big)$\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial e}{\\partial \\theta} = \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial \\theta} + \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial h_2} \\times \\frac{\\partial h_2}{\\partial \\theta} + \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial h_2} \\times \\frac{\\partial h_2}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial \\theta}$\n",
    "\n",
    "We can see that the prefix of the summed derivatives match up, and so once again, the most efficient way to compute the derivatives is by doing a **backward** pass, memoizing the results for the next set of parameters.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Gradient vanishing and explosion\n",
    "\n",
    "The longer the dependency, the more difficult it will be to transmit information via the hidden state. To see why, we can isolate the component of the gradient of $\\theta$ that comes from $x_0$ is equal to: \n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial e}{\\partial h_3} \\times \\frac{\\partial h_3}{\\partial h_2} \\times \\frac{\\partial h_2}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial \\theta}$\n",
    "\n",
    "If the function $f$ was linear, we could summarize this as:\n",
    "\n",
    "&emsp; $f(x, h) = W \\begin{pmatrix} x \\\\ h \\end{pmatrix} = \\begin{pmatrix} W_{11} & W_{12} \\\\ W_{21} & W_{22} \\end{pmatrix} \\begin{pmatrix} x \\\\ h \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $h_n = W_{21} x_{n-1} + W_{22} h_{n-1}$\n",
    "\n",
    "&emsp; $h_n = W_{21} x_{n-1} + W_{22} h_{n-1} = W_{21} x_{n-1} + W_{22} (W_{21} x_{n-2} + W_{22} h_{n-2})$\n",
    "\n",
    "We see that the $n^{th}$ value of $h$ is function of $W_{22}^n$ with respect to $h_0$. $W_{22}$ can be diagonalized into $W_{22} = V \\Lambda V^{-1}$ since it is squared, and so $W_{22}^n = V \\Lambda^n V^{-1}$ and so the eigen values will either explode or vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Structuring a Neural Net\n",
    "----\n",
    "\n",
    "* any structure as long as derivable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
