{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import *\n",
    "from scipy.stats import *\n",
    "from scipy.special import *\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Bayesian Regression\n",
    "---\n",
    "\n",
    "Where we step away from simple point estimates on the parameters of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Full Bayesian Regression\n",
    "\n",
    "In a regression task, we try to estimate the distribution $p(t|x,\\mathcal{D})$ where $\\mathcal{D}$ is the training data set. If we use a parametric method, we have to optimize our model with respect to the parameters $w$. These parameters themselves depend on a prior distribution with parameters $\\alpha$. We also have to take into account the noise on the training data with parameter $\\beta$.\n",
    "\n",
    "In typical MAP regression, we try different combinations of $\\alpha$ and $\\beta$ and maximize the probability $p(w|\\mathcal{D},\\alpha,\\beta)$ with respect to $w$, to produce a point estimate of $w$ that we will use to do future predictions. To do a full Bayesian regression, we would instead need to marginalize over $w$ as well as the hyperparameters $\\alpha$ and $\\beta$ to do predictions:\n",
    "\n",
    "&emsp; $\\displaystyle p(t|x,\\mathcal{D}) = \\iiint p(t|w,\\beta) \\; p(w|\\mathcal{D},\\alpha,\\beta) \\; dw \\; d\\alpha \\; d\\beta$\n",
    "&emsp; where\n",
    "&emsp; $p(w|\\mathcal{D},\\alpha,\\beta) \\propto p(\\mathcal{D}|w,\\beta) \\; p(w|\\alpha)$\n",
    "\n",
    "To better see these kind of dependencies and see the marginalization, it is recommended to rely on **graphical models** such as **Bayesian networks** in that specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Evidence approximation\n",
    "\n",
    "Because this probability distribution is intractable, we usually resort to doing point estimates for $\\alpha$ and $\\beta$ and doing a restricted Bayesian inference where we only marginalize on $w$:\n",
    "\n",
    "&emsp; $\\displaystyle p(t|x,\\mathcal{D},\\alpha,\\beta) = \\int p(t|w,\\beta) \\; p(w|\\mathcal{D},\\alpha,\\beta) \\; dw$\n",
    "\n",
    "Similarly to what we do for $w$ with ML, we will estimate $\\alpha$ and $\\beta$ by maximizing the likelihood of the data with respect to them. Because it follows a maximum likelihood approach at a higher level, this is called **type 2 maximum likelihood** or more often **evidence approximation**:\n",
    "\n",
    "&emsp; $\\alpha^*, \\beta^* = \\underset{\\alpha, \\beta}{\\text{argmax}} p(\\mathcal{D}|\\alpha,\\beta)$\n",
    "&emsp; where\n",
    "&emsp; $\\displaystyle p(\\mathcal{D}|\\alpha,\\beta) = \\int p(\\mathcal{D},w|\\alpha,\\beta) \\; dw = \\int p(\\mathcal{D}|w,\\beta) \\; p(w|\\alpha) \\; dw$\n",
    "\n",
    "We can see here that $w$ behaves as **latent variables**. We can therefore use the EM algorithm to find point estimates for $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Ex: Linear regression (exact inference)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - use EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Ex: Linear regression (approximate inference)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MCMC\n",
    "# Use variational methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
