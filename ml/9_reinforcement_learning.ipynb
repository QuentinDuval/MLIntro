{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Motivation\n",
    "---\n",
    "\n",
    "First check the writings on the markov decision processes.\n",
    "\n",
    "Processus of self improvement based on trial an error:\n",
    "\n",
    "* Interact with a complex environment by trying actions\n",
    "* Get rewards (which correspond to an objective)\n",
    "* Ability to observe the environment through sensors (or sometimes actions)\n",
    "* Assumption that a latent variable *s* (the state) produces the observations through a sensor model\n",
    "* Assumption that the next state is decided by the previous state + action (and randomness)\n",
    "\n",
    "Find a policy $\\pi(s)$ at state *s* that returns  maximizes the reward over time. This policy transforms long term planning into a simple greedy policy in a reflex agent (no consideration of time but to estimate the state *s* and just pick the best action)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Design of an agent\n",
    "---\n",
    "\n",
    "The agent does not necessarily have to learn the full policy of a complex agent. The agent might be composed of many parts, some of which are deterministic/stochastic classical programming, some of which are learned by Reinforcement Learning (RL) or Supervised Learning (SL). See the design of AlphaGo which mixes: SL, RL and Monte Carlo Tree Search.\n",
    "\n",
    "For instance, an agent might used SL for the perception only and take rational decisions based on standard algorithms. An agent might use RL to evaluate a state value and then use an algorithm such as minimax / monte carlo tree search to do **planning** and take the most valuable branch. An agent might use RL to translate high level orders into low level actions (such as moving a robot but the instructions are coded classically).\n",
    "\n",
    "You have to frame your problem, decompose it, identify what parts need learning. The overall AGI that does all of us is not yet there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Deep Reinforcement Learning\n",
    "---\n",
    "\n",
    "Learning the action-value or state-value exactly is complex due to the state explosion in many games. Even if the states are tractable, we are often interested in generalization (learn from similar situations and therefore borrow the statistical strength of many more samples).\n",
    "\n",
    "Deep Neural Networks are one of the powerful way to have great approximation and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
