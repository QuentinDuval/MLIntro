{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show that the classification program leads to P(Ck|x) = softmax avec a(k) = ln P(x|Ck)P(Ck)\n",
    "# Talk about the generative (model the process generating data) vs discriminative (logistic regression here) classification\n",
    "# Show that the number of parameters to adjust in each case is not of the same (quadratic in dimensions for generative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General approach\n",
    "---\n",
    "\n",
    "* Discriminative\n",
    "* Generative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative\n",
    "---\n",
    "\n",
    "In the generative approach, we try to find an appropriate data generation process for the different classes we are interested in:\n",
    "\n",
    "&emsp; $p(x|C_k), \\forall k$.\n",
    "\n",
    "Then we use these **forward probabilities** (probability of effect knowing the cause) to compute the **backward probabilities** (probability of cause knowing the effect, that is in our case, the probability that point $x$ belongs to class $C_k$), using Bayes' formula:\n",
    "\n",
    "&emsp; $p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{\\sum_i p(x|C_i)p(C_i)}$\n",
    "\n",
    "Then we can take the $C_k$ with the highest probability, which corresponds to MAP (Maximum A Posteriory) and not ML (Maximum Likelihood). A full Bayesian treatment would also include prior on the parameters (discussed later).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The motivation for doing this is that the forward probabilities are usually:\n",
    "\n",
    "* more stable than backward probabilities which depends on other factors, like the priors of each classes\n",
    "* more intuitive for humans than backward probabilities, because their correspond to causal direction\n",
    "\n",
    "For instance, in the case of the diagnostic of a disease:\n",
    "\n",
    "* forward probabilities are the false positive and false negative rate of the diagnostic tool\n",
    "* backward probabilities inform of the probability of having the disease given the diagnostic\n",
    "* backward probabilities (evidence => cause) are what we want, but they depend on the prior $P(disease)$\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "The big disadvantage is that forward probabilities can be pretty complex to estimate, and will usually require quite a lot more of parameters to estimate. For instance, if $x$ is M-dimensional, fitting a gaussian for a single class will require:\n",
    "\n",
    "* up to $\\frac{M(M+1)}{2}$ parameters for the covariance matrix\n",
    "* up to $M$ parameters for the mean\n",
    "\n",
    "In comparison, a simple logistic regression model will only need to adjust $M$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax, and Linear models\n",
    "\n",
    "Following our generative perspective, if $p(x|C_k)$ is gaussian: $p(x|C_k) = \\frac{1}{(2 \\pi)^{D/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}$\n",
    "\n",
    "Thus we can rewrite Bayes' rule like this:\n",
    "\n",
    "&emsp; $p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{\\sum_i p(x|C_i)p(C_i)} = \\frac{exp(a_k)}{\\sum_i exp(a_i)}$\n",
    "\n",
    "Where we have:\n",
    "\n",
    "&emsp; $a_k = \\log p(x|C_k)p(C_k) = \\log p(C_k) - \\frac{1}{2} \\log ((2 \\pi)^D |\\Sigma|) -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)$\n",
    "\n",
    "&emsp; $a_k = \\log p(C_k) - \\frac{D}{2} \\log 2 \\pi - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (x^T \\Sigma^{-1} x + \\mu^T \\Sigma^{-1} \\mu - 2 \\mu^T \\Sigma^{-1} x) $\n",
    "\n",
    "We can simplify the terms by first removing the common part (which will cancel out as common factors) and, if we assume all classes to have the same covariance matrix, get this form, where the quadratic terms have vanished:\n",
    "\n",
    "&emsp; $a_k = \\log p(C_k) - \\frac{1}{2}(\\mu^T \\Sigma^{-1} \\mu - 2 \\mu^T \\Sigma^{-1} x)$\n",
    "\n",
    "We thereform got a linear form in $x$, which we can put back to the softmax function, to finally get:\n",
    "\n",
    "&emsp; $p(C_k|x) = \\frac{exp(w_k^T x)}{\\sum_i exp(w_i^T x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminiative\n",
    "---\n",
    "\n",
    "Apply maximum likelihood on the parameters $w$ for the softmax formula: and you get the notion of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
