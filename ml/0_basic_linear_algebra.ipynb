{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Column space & matrix multiplication\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Column space\n",
    "\n",
    "The \"good\" way to see matrix multiplication: take each column of the right matrix and use these numbers as a linear combinations of the column vectors of the matrix.\n",
    "\n",
    "&emsp; $\\begin{pmatrix} u_1 & u_2 & ... & u_n \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{pmatrix} = x_1 u_1 + x_2 u_2 + ... + x_n u_n$\n",
    "\n",
    "The **column vectors are the projection of the current basis vectors, encoded in the basis of the source space**. Following this recipee it is easy to compute the rotation matrix of angle $\\theta$. The first basis vector should land at $(\\cos \\theta, \\sin \\theta)$, and the second vector should land at $(-\\sin \\theta, \\cos \\theta)$. We show this for several other matrices:\n",
    "\n",
    "&emsp; $rot(\\theta) = \\begin{pmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix}$\n",
    "&emsp;\n",
    "&emsp; $perm(0,1) = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$\n",
    "&emsp;\n",
    "&emsp; $scale(1,\\lambda) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\lambda \\end{pmatrix}$\n",
    "\n",
    "This is much easier than trying to reason the opposite way, and find that the new first component should be $\\cos \\theta - \\sin \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Matrix multiplication\n",
    "\n",
    "The formula above is also valid in case $x_i$ are replaced by row vectors $v_i^T$:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} u_1 & u_2 & ... & u_n \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ ... \\\\ v_n^T \\end{pmatrix} = u_1 v_1^T + u_2 v_2^T + ... + u_n v_n^T$\n",
    "\n",
    "We therefore have two views for the multiplication of $A \\in \\mathbb{R}^{m \\times k}$ and $B \\in \\mathbb{R}^{k \\times n}$:\n",
    "\n",
    "&emsp; $(AB)_{ij} = \\sum_k A_{ik} B_{kj} = row(A,i)^T \\times col(B,j)$\n",
    "\n",
    "&emsp; $AB = \\sum_k col(A,k) \\times row(B,k) = $ sum of rank 1 matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Change of basis\n",
    "\n",
    "Vectors are abstract entities, but we often represent them as a list of numbers, which highly depend on the choice of basis. The same goes for linear transformations $\\phi(x): \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, which can be encoded as a matrix $A \\in \\mathbb{R}^{m \\times n}$ in a specific basis.\n",
    "\n",
    "If we have a vector $x$ encoded in a basis $V$, and we want to encode this vector in the basis $U$, and we have the representation of the vectors of $U = (u_1, \\dots u_n)$ in $V$. We look for the representation of the vector $x$ in $U$ such that $x = \\sum_i y_i u_i$:\n",
    "\n",
    "&emsp; $x = \\sum y_i u_i$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $x = U y$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $y = U^{-1} x$\n",
    "&emsp; where\n",
    "&emsp; $y$ is the representation of $x$ in $U$\n",
    "\n",
    "Similarly, if $\\phi(x)$ is encoded by matrix $A$ in the basis $V$, then if we take a vector expressed in basis $U$, the apply $A$, then move back this vector in $U$, we get the encoding of $\\phi(x)$ in $U$:\n",
    "\n",
    "&emsp; $\\phi$ is $A$ in basis $V$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\phi$ is $B = U^{-1} A U$ in basis $U$\n",
    "&emsp; where $U$ is encoded in basis $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Row space and inner products\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dual Space / Row space\n",
    "\n",
    "Formally, given a vector space $V$, its dual space $V^*$ is the **space of the linear transformations** $L(V,\\mathbb{R})$ from $V$ to $\\mathbb{R}$. This dual space is also a linear space, and has the same dimensionality as $V$:\n",
    "\n",
    "&emsp; $\\{x_n\\}$ basis of $V \\implies \\{w_n\\}$ basis of $V^*$\n",
    "&emsp; where\n",
    "&emsp; $w_j(x_i) = \\delta_{ij}$\n",
    "\n",
    "Linear transformations from $\\mathbb{R}^N$ to $\\mathbb{R}$ of $V^*$ can be encoded as matrices $(1,N)$, that is as **row vectors**. We can therefore see the dual space $V^*$ as being the column space and the basis of ${w_n}$ as spanning this row space.\n",
    "\n",
    "&emsp; $V = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $w_1^T V = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\begin{pmatrix} w_1^T \\\\ w_2^T \\\\ w_3^T \\end{pmatrix} V = I$\n",
    "&emsp; which simply translates $w_j(x_i) = \\delta_{ij}$\n",
    "\n",
    "The basis of $V^*$ is therefore $W^T = (w_1, w_2, w_3)$ such that $W = V^{-1}$. Note that from $w_j(x_i) = \\delta_{ij}$, we can see that **the dual basis of an orthogonal basis $V$ are the row vectors $V^T$** since each vector satisfies $v_i^T v_j = \\delta_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dot product, Inner products and bilinear forms\n",
    "\n",
    "The dot product between $x$ and $y$, written $x \\cdot y$, can also be written as $x^T y$, which shows a deep connection between the row space and the column space: operations such as the dot product between two vectors in the column space, can be writen as linear operation, that is elements of the dual space (row space) and encoded as $\\mathbb{R}^{1 \\times n}$ matrices.\n",
    "\n",
    "Similarly, **matrices can be views as a stack of dot products** each applied on the input vector. This view is the one that is used to define the matrix multiplication as $y_i = \\sum_i a_{ij} x_j$. This view is also useful when taking about the Jacobian of a matrix.\n",
    "\n",
    "The notion of dot product can in fact be generalized to the notion of **inner product**, noted $\\langle x, y \\rangle$. An inner product is a bilinear form, a function, linear in both its arguments, which is both:\n",
    "\n",
    "* Symmetric: &emsp; &emsp; &emsp; $\\langle x, y \\rangle = \\langle y, x \\rangle$\n",
    "* Positive definite: &emsp; $\\, \\langle x, x \\rangle \\ge 0$ and $\\langle x, x \\rangle = 0 \\iff x = 0$\n",
    "\n",
    "Bilinear forms can be encoded as **symmetric positive definite matrices**  $\\langle x, y \\rangle = x^T S y$:\n",
    "\n",
    "* Symmetric: &emsp; &emsp; &emsp; $x^T S y = y^T S x \\implies S = S^T$\n",
    "* Positive definite: &emsp; $\\, x^T S x \\ge 0$ and $x^T S x = 0 \\iff x = 0$\n",
    "\n",
    "The dot product is itself a very specific form of inner product in which the matrix is the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### N-linear forms?\n",
    "\n",
    "A dot product $x \\cdot y$ can be viewed as a linear form taking $y$ and returning $x^T y$. It can also be viewed as a bilinear form taking $x$ and $y$ and returning $x^T I y$. The concept can be generalized for N-arry forms, if we replace 2-dimensional matrices by N-dimensional *tensor* (where tensor here just means multi-dimentional arrays).\n",
    "\n",
    "For instance, a trilinear form could be encoded as $A \\in \\mathbb{R}^{N \\times N \\times N}$, but there would not be a nice matrix multiplication formulation to easily write it. We could also generalize it as $A \\in \\mathbb{R}^{M \\times N \\times P}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "---\n",
    "\n",
    "Finding the eigen values and eigen vectors is only possible on **square matrices**. The recipee is to proceed in two stages:\n",
    "\n",
    "1. find the eigen values of the matrix\n",
    "2. find the associated eigen vectors\n",
    "\n",
    "To find the eigen values, we need to find the roots of the **characteristic polynomial** $det(A-\\lambda I)=0$.\n",
    "\n",
    "&emsp; $Av = \\lambda v \\implies Av = \\lambda I v \\implies (A - \\lambda I)v = 0 \\implies det(A-\\lambda I)=0$ (because $A - \\lambda I$ as a non-zero null space)\n",
    "\n",
    "After the factorization, we get:\n",
    "\n",
    "&emsp; $det(A-\\lambda I) = (\\lambda - \\lambda_0)^{\\mu_0} \\times ... \\times (\\lambda - \\lambda_k)^{\\mu_k}$\n",
    "\n",
    "The **degree of each root** $\\mu_i$ gives us the number of **dimensions of the eigenspace** associated to the eigenvalue $\\lambda_i$, and therefore the number of linearly independent eigenvectors associated with the given eigenvalue. Once we identified the eigenvalues $\\lambda_i$, we can find the associated eigenvectors $v_i$ by solving $Av = \\lambda_i v$ for $v$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\implies det(A - \\lambda I) = (2 - \\lambda)^2 - 1 = 0 \\implies \\lambda = 2 \\pm 1$\n",
    "\n",
    "To find the eigen vector associated to the eigenvalue $\\lambda_0 = 1$, we solve the following system:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\Â y \\end{pmatrix} = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\implies x = -y \\implies v_0 = (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$\n",
    "\n",
    "To find the eigen vector associated to the eigenvalue $\\lambda_1 = 3$, we solve the following system:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\Â y \\end{pmatrix} = 3 \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\implies x = y \\implies v_1 = (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 1.]), array([[ 0.70710678, -0.70710678],\n",
       "        [ 0.70710678,  0.70710678]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [2, 1],\n",
    "    [1, 2]])\n",
    "\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**todo - show the interpretation as column vectors, see: https://www.youtube.com/watch?v=or6C4yBk_SY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Complex and real eigenvalues\n",
    "\n",
    "**todo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Matrix identities\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Useful multiplications\n",
    "\n",
    "&emsp; $X = \\begin{pmatrix} x_1^T \\\\ x_2^T \\\\ ... \\\\ x_n^T \\end{pmatrix} = \\text{design matrix} \\in \\mathcal{R}^{n \\times p}$\n",
    "\n",
    "&emsp; $\\displaystyle X^T X = \\sum x_n x_n^T = \\text{covariance matrix} \\in \\mathcal{R}^{p \\times p} = \\text{all dimensions against each other for each sample}$\n",
    "\n",
    "&emsp; $(X X^T)_{ij} = x_i^T x_j = \\text{kernel / similarly matrix} \\in \\mathcal{R}^{n \\times n} = \\text{all samples against each other}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pseudo-inverse of a matrix\n",
    "\n",
    "If a matrix has more rows that it has columns, we can define what we call its **left pseudo-inverse**:\n",
    "\n",
    "&emsp; $A^{\\dagger} = (A^T A)^{-1} A^T$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A^{\\dagger} A = (A^T A)^{-1} A^T A = I$\n",
    "\n",
    "Similarly, if a matrix has more columns that it has rows, we can define what we call its **right pseudo-inverse**:\n",
    "\n",
    "&emsp; $A^{\\dagger} = A^T (A A^T)^{-1}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A A^{\\dagger} = A A^T (A A^T)^{-1} = I$\n",
    "\n",
    "**The pseudo-inverse does not always exists**: the $A^T A$ and $A A^T$ matrices are square and symmetric and positive semi-definite, but might still have null eigenvalues (to see it, just multiply a column vector by a row vector, its transpose).\n",
    "\n",
    "A nice property of pseudo-inverse matricies is that **the pseudo-inverse is equal to the inverse if the inverse exists**:\n",
    "\n",
    "&emsp; $A^{\\dagger} = (A^T A)^{-1} A^T$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A^{\\dagger} = A^{-1} (A^T)^{-1} A^T = A^{-1}$\n",
    "\n",
    "&emsp; $A^{\\dagger} = A^T (A A^T)^{-1}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A^{\\dagger} = A^T (A^T)^{-1} A^{-1} = A^{-1}$\n",
    "\n",
    "*Link:* https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
