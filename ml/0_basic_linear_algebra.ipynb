{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Column and row spaces\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Column space\n",
    "\n",
    "The \"good\" way to see matrix multiplication: take each column of the right matrix and use these numbers as a linear combinations of the column vectors of the matrix.\n",
    "\n",
    "&emsp; $\\begin{pmatrix} u_1 & u_2 & ... & u_n \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_n \\end{pmatrix} = x_1 u_1 + x_2 u_2 + ... + x_n u_n$\n",
    "\n",
    "We can see the column space as the space generated by column vectors, and we can see the **column vectors as the projection of the current basis vectors, encoded in the basis of the source space**.\n",
    "\n",
    "Note that the formula above is also valid in case $x_i$ are replaced by row vectors $v_i^T$:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} u_1 & u_2 & ... & u_n \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ ... \\\\ v_n^T \\end{pmatrix} = u_1 v_1^T + u_2 v_2^T + ... + u_n v_n^T$\n",
    "\n",
    "It shows that we can decompose any matrix multiplication $AB$ where $A \\in R^{m \\times k}$ and $B \\in R^{k \\times n}$, as the sum of the product of the columns of the first matrix with the rows of the second matrix:\n",
    "\n",
    "&emsp; $AB = \\sum_k k^{th}\\text{ col of A} \\times k^{th}\\text{ row of B} = \\sum_k A_{[:,k]} B_{[k,:]} = \\sum_k \\text{rank 1 matrices}$\n",
    "\n",
    "Which is compliant to the classical rule: $(AB)_{ij} = \\sum_k A_{ik} B_{kj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Column space rank\n",
    "\n",
    "We can find the rank of a matrix by looking at its columns one by one, adding them to a new matrix if they are not a linear combinations of the other columns. For each column added, we complete a matrix that gives the linear combinations if they exist. This is illustrated below:\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 2 & 1 & 3 \\\\ 3 & 2 & 5 \\\\ 5 & 7 & 12 \\end{pmatrix} \\rightarrow A = \\begin{pmatrix} 2 & 1 \\\\ 3 & 2 \\\\ 5 & 7 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} = C R$\n",
    "\n",
    "$R$ represent the linear combinations necessary to build the matrix $A$ from the matrix $C$:\n",
    "\n",
    "1. take the first row of C\n",
    "2. take the second row of C\n",
    "3. take the sum of the first and second row of C\n",
    "\n",
    "Interestingly, we can read this product the other way around, as $C^T$ being the linear combinations to perform on the rows of $R$ to obtain the matrix $A$ (this can easily be seen by transposing the product):\n",
    "\n",
    "1. take 2 times the first row of $R$ plus 1 times the second row of $R$\n",
    "2. take 3 times the first row of $R$ plus 2 times the second row of $R$\n",
    "3. take 5 times the first row of $R$ plus 7 times the second row of $R$\n",
    "\n",
    "This demonstrates one big theorem of Linear Algebra: the rank of the column space of $A$ is equal to the rank of the row space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dual Space / Row space\n",
    "\n",
    "Formally, given a vector space $V$, its dual space $V^*$ is the space of the linear transformations $L(V,\\mathcal{R})$ from $V$ to $\\mathcal{R}$. This dual space is also a linear space, and has the same dimensionality as $V$:\n",
    "\n",
    "&emsp; $\\{x_n\\}$ basis of $V \\implies \\{w_n\\}$ basis of $V^*$\n",
    "&emsp; where\n",
    "&emsp; $w_j(x_i) = \\delta_{ij}$\n",
    "\n",
    "In the specific case of finite space $V$, **linear transformations** from $V = \\mathcal{R}^N$ to $\\mathcal{R}$ of $V^*$ can be encoded as matrices $(1,N)$, that is as **row vectors**, while elements from the vector space $V$ are encoded as column vectors. We can therefore see the dual space of the row space as being the column space (and inversely) and a basis of a row space as being a basis of the dual space.\n",
    "\n",
    "Let us compute the basis $W^T = (w_1, w_2, w_3)$ of the dual space $V^*$ of the vector space spanned by $V = (v_1, v_2, v_3)$:\n",
    "\n",
    "&emsp; $V = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $w_1^T V = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $W V = \\begin{pmatrix} w_1^T \\\\ w_2^T \\\\ w_3^T \\end{pmatrix} V = I$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $W = V^{-1}$\n",
    "\n",
    "The basis of the dual space of $V$ is therefore the collection of row vectors $W$ such that $W = V^{-1}$, which is basically the definition imposed by $w_j(x_i) = \\delta_{ij}$, which is basically the recipe for an identity matrix. We can also see from this definition that **the dual basis of an orthogonal basis $V$ are the row vectors $V^T$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5 -0.5  0.5]\n",
      " [ 0.5  0.5 -0.5]\n",
      " [-0.5  0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 1]])\n",
    "\n",
    "W = np.linalg.inv(A.T)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We could also solve for the system by Gaussian eliminiation for $V^T W^T = I$:\n",
    "\n",
    "&emsp; $V^T w_i = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} a \\\\ b \\\\ c-a+b \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $W^T = \\begin{pmatrix} 0.5 & -0.5 & 0.5 \\\\ 0.5 & 0.5 & -0.5 \\\\ -0.5 & 0.5 & 0.5 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Change of basis\n",
    "\n",
    "We have a vector $x$ and basis in $U = (u_1, \\dots u_n)$, we look for the representation of the vector $x$ in $U$.\n",
    "\n",
    "The somewhat intuitive but wrong answer is that we should project $x$ on each vector $u_i$, but this answer is obviously wrong: for instance, if we double the length of $u_i$, we expect the projection on $u_i$ to be halved (and not double if we do a dot product).\n",
    "\n",
    "Instead, if we look at the problem correct, we need to look for the coefficient $y_i$ such that $x = \\sum_i y_i u_i$ (the components of $u_i$), which means $x = U y$. The projection of $x$ on $U$ is therefore $y = U^{-1} x$.\n",
    "\n",
    "It means that the decomposition $A = V \\Lambda V^{-1}$ can be viewed as mapping the input vector on the base $V$, scaling it via the diagonal matrix $\\Lambda$, then mapping it back on its initial base. It also show that matrices are transformations expressed on a given basis: $\\Lambda$ is the same transformation as $A$ but in the basis $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Matrix identities\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Two views of matrix multiplication\n",
    "\n",
    "&emsp; $AB = \\sum_k col(A,k) \\times row(B,k) = \\sum_k \\text{rank 1 matrices}$\n",
    "\n",
    "&emsp; $(AB)_{ij} = \\sum_k A_{ik} B_{kj} = row(A,i)^T col(B,j)$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Useful multiplications\n",
    "\n",
    "&emsp; $X = \\begin{pmatrix} x_1^T \\\\ x_2^T \\\\ ... \\\\ x_n^T \\end{pmatrix} = \\text{design matrix} \\in \\mathcal{R}^{n \\times p}$\n",
    "\n",
    "&emsp; $\\displaystyle X^T X = \\sum x_n x_n^T = \\text{covariance matrix} \\in \\mathcal{R}^{p \\times p} = \\text{all dimensions against each other for each sample}$\n",
    "\n",
    "&emsp; $(X X^T)_{ij} = x_i^T x_j = \\text{kernel / similarly matrix} \\in \\mathcal{R}^{n \\times n} = \\text{all samples against each other}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pseudo-inverse of a matrix\n",
    "\n",
    "If a matrix has more rows that it has columns, we can define what we call its **left pseudo-inverse**:\n",
    "\n",
    "&emsp; $A^{\\dagger} = (A^T A)^{-1} A^T$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A^{\\dagger} A = (A^T A)^{-1} A^T A = I$\n",
    "\n",
    "Similarly, if a matrix has more columns that it has rows, we can define what we call its **right pseudo-inverse**:\n",
    "\n",
    "&emsp; $A^{\\dagger} = A^T (A A^T)^{-1}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A A^{\\dagger} = A A^T (A A^T)^{-1} = I$\n",
    "\n",
    "**The pseudo-inverse does not always exists**: the $A^T A$ and $A A^T$ matrices are square and symmetric and positive semi-definite, but might still have null eigenvalues (to see it, just multiply a column vector by a row vector, its transpose).\n",
    "\n",
    "A nice property of pseudo-inverse matricies is that **the pseudo-inverse is equal to the inverse if the inverse exists**:\n",
    "\n",
    "&emsp; $A^{\\dagger} = (A^T A)^{-1} A^T$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A^{\\dagger} = A^{-1} (A^T)^{-1} A^T = A^{-1}$\n",
    "\n",
    "&emsp; $A^{\\dagger} = A^T (A A^T)^{-1}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $A^{\\dagger} = A^T (A^T)^{-1} A^{-1} = A^{-1}$\n",
    "\n",
    "*Link:* https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Derivation of matrices\n",
    "\n",
    "**todo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "---\n",
    "\n",
    "Finding the eigen values and eigen vectors is only possible on **square matrices**. The recipee is to proceed in two stages:\n",
    "\n",
    "1. find the eigen values of the matrix\n",
    "2. find the associated eigen vectors\n",
    "\n",
    "To find the eigen values, we need to find the roots of the **characteristic polynomial** $det(A-\\lambda I)=0$.\n",
    "\n",
    "&emsp; $Av = \\lambda v \\implies Av = \\lambda I v \\implies (A - \\lambda I)v = 0 \\implies det(A-\\lambda I)=0$ (because $A - \\lambda I$ as a non-zero null space)\n",
    "\n",
    "After the factorization, we get:\n",
    "\n",
    "&emsp; $det(A-\\lambda I) = (\\lambda - \\lambda_0)^{\\mu_0} \\times ... \\times (\\lambda - \\lambda_k)^{\\mu_k}$\n",
    "\n",
    "The **degree of each root** $\\mu_i$ gives us the number of **dimensions of the eigenspace** associated to the eigenvalue $\\lambda_i$, and therefore the number of linearly independent eigenvectors associated with the given eigenvalue. Once we identified the eigenvalues $\\lambda_i$, we can find the associated eigenvectors $v_i$ by solving $Av = \\lambda_i v$ for $v$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\implies det(A - \\lambda I) = (2 - \\lambda)^2 - 1 = 0 \\implies \\lambda = 2 \\pm 1$\n",
    "\n",
    "To find the eigen vector associated to the eigenvalue $\\lambda_0 = 1$, we solve the following system:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\implies x = -y \\implies v_0 = (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$\n",
    "\n",
    "To find the eigen vector associated to the eigenvalue $\\lambda_1 = 3$, we solve the following system:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = 3 \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\implies x = y \\implies v_1 = (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 1.]), array([[ 0.70710678, -0.70710678],\n",
       "        [ 0.70710678,  0.70710678]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [2, 1],\n",
    "    [1, 2]])\n",
    "\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Matrix decompositions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LU decomposition\n",
    "\n",
    "A matrix $A$ can be decomposed as $A = P L U$ where $P$ is a permutation matrix, $L$ is a lower diagonal matrix, $U$ is a upper diagonal matrix. The permutation matrix is not necessary. Example:\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 1 & 2 & -1 \\\\ 4 & 5 & 0 \\\\ -2 & -1 & 4 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ 0.25 & 0.5 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 5 & 0 \\\\ 0 & 1.5 & 4 \\\\ 0 & 0 & -3 \\end{pmatrix}$\n",
    "\n",
    "The goal of this decomposition $A = LU$ is to **facilitate the resolution of systems of linear equations** $Ax = b$. Indeed, we can solve these systems using Gaussian Elimination, but this requires $O(N^3)$ steps, with $N$ being the number of rows or columns, because of $O(N)$ row manipulations that each go through half the matrix in average.\n",
    "\n",
    "In case of repeating evaluation for different $b$, decomposing the matrix $A = PLU$ allows to solve the system more quickly. Indeed, we can solve the system by exploiting the fact that $Ax = P(L(Ux)) = b$:\n",
    "\n",
    "1. Solve $Pz = b$: we can just permute the rows of $b$ in $O(N)$\n",
    "2. Solve $Ly = z$: we can get the value of $y$ in $O(N^2)$ by substitution since the gaussian elimination is already done\n",
    "3. Solve $Ux = y$: we can get the value of $x$ in $O(N^2)$ by substitution since the gaussian elimination is already done\n",
    "\n",
    "The idea of the LU decomposition is to see $Ax = b$ as $IAx = b$ and then perform scaling or row addition operations on $A$ as we do for Gaussian elimination, which can be represented as matrices $E_i$, and apply the reverse transformation on $I$:\n",
    "\n",
    "&emsp; $IAx=b \\implies (IE_0^{-1})(E_0A)x = b \\implies (E_0^{-1} .. E_n^{-1})(E_n .. E_0 A)x = b \\implies LUx=b$\n",
    "\n",
    "The technical trick we can use to do so is to draw the matrices $A$ and $I$ side by side, and doing the reverse operations:\n",
    "\n",
    "* divising by $k$ the row of $I$ when $A$'s row is multiplied by $k$\n",
    "* substracting row $k$ to row $j$ in $I$ when $A$'s row $k$ is adding to row $j$ (only for eliminated column)\n",
    "\n",
    "**Example**:\n",
    "\n",
    "We start with our $U$ matrix being equal to $A$, and $L$ as the identity matrix:\n",
    "\n",
    "&emsp; $U = \\begin{pmatrix} 1 & 2 & -1 \\\\ 4 & 5 & 0 \\\\ -2 & -1 & 4 \\end{pmatrix} L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n",
    "\n",
    "Gaussian elimination of the first column:\n",
    "\n",
    "&emsp; $U = \\begin{pmatrix} 1 & 2 & -1 \\\\ 0 & -3 & 4 \\\\ 0 & 3 & 2 \\end{pmatrix} L = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\boldsymbol{4} & 1 & 0 \\\\ \\boldsymbol{-2} & 0 & 1 \\end{pmatrix}$\n",
    "\n",
    "Gaussian elimination of the second column:\n",
    "\n",
    "&emsp; $U = \\begin{pmatrix} 1 & 2 & -1 \\\\ 0 & -3 & 4 \\\\ 0 & 0 & 6 \\end{pmatrix} L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ -2 & \\boldsymbol{-1} & 1 \\end{pmatrix}$\n",
    "\n",
    "**Note:** The LU decomposition can also be used to compute the determinant faster (because the determinant is factorial complexity in general, but linear complexity for diagonal or triangular matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "[[ 1.    0.    0.  ]\n",
      " [-0.5   1.    0.  ]\n",
      " [ 0.25  0.5   1.  ]]\n",
      "[[ 4.   5.   0. ]\n",
      " [ 0.   1.5  4. ]\n",
      " [ 0.   0.  -3. ]]\n",
      "[[ 1.  2. -1.]\n",
      " [ 4.  5.  0.]\n",
      " [-2. -1.  4.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 2, -1],\n",
    "    [4, 5, 0],\n",
    "    [-2, -1, 4]])\n",
    "\n",
    "permutation, lower_diag, upper_diag = scipy.linalg.lu(A)\n",
    "print(permutation)\n",
    "print(lower_diag)\n",
    "print(upper_diag)\n",
    "print(permutation @ lower_diag @ upper_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**todo - show the interpretation as column vectors, see: https://www.youtube.com/watch?v=or6C4yBk_SY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### QR decomposition\n",
    "\n",
    "Decomposition of the matrix $A = QR$ as an orthogonal matrix $Q$ and a upper right triangular matrix $R$.\n",
    "\n",
    "This decomposition helps solving system of linear equations: $Ax = b$ becomes $Rx = Q^T b$ (because the inverse of an orthogonal matrix is its transpose) which is easier to solve since $R$ is triangular. If noise *n* is added: $Ax = b + n$ becomes $Rx = Q^T b + Q^T n$, which keeps some nice properties of the covariance matrix of the noise:\n",
    "\n",
    "&emsp; $E[n n^T] = \\text{covariance of noise} = \\sigma^2 I \\implies E[Q^T n n^T Q] = Q^T E[n n^T] Q = \\sigma^2 I$\n",
    "\n",
    "In case of white independent noise across dimension, this property is pretty interesting.\n",
    "\n",
    "**Gram-Schmidt process:**\n",
    "\n",
    "&emsp; $A = (a_1, a_2, ... a_n)$\n",
    "&emsp; $Q = (e_1, e_2, ... e_n)$\n",
    "\n",
    "Initialization phase:\n",
    "\n",
    "&emsp; $\\displaystyle e_1 = \\frac{a_1}{\\Vert a_1 \\Vert} \\implies a_1 = \\langle a_1, e_1 \\rangle e_1$\n",
    "\n",
    "Then for each $a_j$ (increasing $j$), remove the projections on the already existing vectors $e_i, \\forall i < j$, and normalize:\n",
    "\n",
    "&emsp; $\\displaystyle e_j = \\frac{u_j}{\\Vert u_j \\Vert} = \\frac{a_j - \\sum_{i=0}^{j-1} \\langle a_j, e_i \\rangle e_i}{\\Vert a_j - \\sum_{i=0}^{j-1} \\langle a_j, e_i \\rangle e_i \\Vert} \\implies a_j = \\sum_{i=0}^{j} \\langle a_j, e_i \\rangle e_i$\n",
    "\n",
    "And we have $R$:\n",
    "\n",
    "&emsp; $\\displaystyle R={\\begin{pmatrix}\\langle e_1, a_1 \\rangle &\\langle e_1,a_2 \\rangle & \\ldots \\\\ 0 & \\langle e_2, a_2 \\rangle & \\ldots \\\\ \\vdots & \\vdots &\\ddots \\end{pmatrix}}$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 3 & 1 \\\\ 4 & 1 \\end{pmatrix}$\n",
    "&emsp; $a_1 = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$\n",
    "&emsp; $a_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "&emsp; $u_1 = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$\n",
    "&emsp; $\\Vert u_1 \\Vert = 5$\n",
    "\n",
    "&emsp; $e_1 = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{4}{5} \\end{pmatrix}$\n",
    "\n",
    "&emsp; $u_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{7}{5} \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{25} \\\\ \\frac{-3}{25} \\end{pmatrix}$\n",
    "&emsp; $\\displaystyle \\Vert u_2 \\Vert = \\frac{1}{5}$\n",
    "\n",
    "&emsp; $e_2 = \\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{-3}{5} \\end{pmatrix}$\n",
    "\n",
    "And so we have:\n",
    "\n",
    "&emsp; $Q = \\begin{pmatrix} \\frac{3}{5} & \\frac{4}{5} \\\\ \\frac{4}{5} & \\frac{-3}{5} \\end{pmatrix}$\n",
    "&emsp; $R = \\begin{pmatrix} 5 & \\frac{7}{5} \\\\ 0 & \\frac{1}{5} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Diagonalisation / Eigen decomposition / Spectral decomposition\n",
    "\n",
    "A matrix of rank $N$ always has $N$ non-zero complex eigen values. Real eigen values correspond to scaling. Complex eigen values correspond to rotations (plus scaling). Such matrices can be decomposed as:\n",
    "\n",
    "&emsp; $A = U \\Lambda U^{-1}$, where $U$ are the eigen vectors, and $\\Lambda$ contains the eigen values $\\Lambda_{ii} = \\lambda_i$\n",
    "\n",
    "Let's prove it! For each eigen vector $v_i$, we have $A v_i = \\lambda_i v_i$. If we assemble these eigen vectors into a matrix, we get the formula:\n",
    "\n",
    "&emsp; $A U = U \\Lambda$ (each column of  Λ  multiplies the only eigen vector it matches) $\\implies A = U \\Lambda U^{-1}$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$ has two eigenvalues $\\lambda_0 = 1$ and $\\lambda_1 = 3$ with eigenvectors $v_0 = (1, -1)$ and $v_1 = (1, 1)$\n",
    "\n",
    "&emsp; $A U = U \\Lambda \\implies \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 3 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3]\n",
      " [-1  3]]\n",
      "[[ 1  3]\n",
      " [-1  3]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [2, 1],\n",
    "    [1, 2]])\n",
    "\n",
    "U = np.array([\n",
    "    [1, 1],\n",
    "    [-1, 1]])\n",
    "\n",
    "D = np.diag([1, 3])\n",
    "\n",
    "print(A @ U)\n",
    "print(U @ D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Symmetric matrices\n",
    "\n",
    "Symmetric matrices have real eigenvalues $\\lambda_i$ and we can always find orthonormal eigenvectors $q_i$ that form a basis of their column space. If we assemble the vectors $q_i$ as column vectors of a matrix $Q$, we have the factorization:\n",
    "\n",
    "&emsp; $S = Q \\Lambda Q^T$, where $\\Lambda$ contains the eigen values $\\Lambda_{ii} = \\lambda_i$\n",
    "\n",
    "Let's prove it! For each eigen vectors $q_i$, we have $S q_i = \\lambda_i q_i$. If we assemble these eigen vectors into a matrix, we get the formula (where $\\Lambda$ is diagonal with $\\Lambda_{ii} = \\lambda_i$):\n",
    "\n",
    "&emsp; $S Q = Q \\Lambda$ (each column of $\\Lambda$ multiplies the only eigen vector it matches)\n",
    "\n",
    "&emsp; $S = Q \\Lambda Q^T$ (since $Q Q^T = I$)\n",
    "\n",
    "Spectral theorem **todo**:\n",
    "\n",
    "&emsp; $S = \\sum_i^N \\lambda_i q_i q_i^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Singular Value Decomposition\n",
    "\n",
    "For any matrix, of any shape, we can always decompose the matrix as: $A = U \\Sigma V^T$, where:\n",
    "\n",
    "* $V$ is orthogonal and contains the eigen values of $A^T A$, with eigen values $\\lambda_i$\n",
    "* $U$ is orthogonal and contains the eigen values of $A A^T$\n",
    "* $\\Sigma$ is diagonal and contains the singular values $\\Sigma_{ii} = \\sigma_i$ such that $\\sigma_i = \\sqrt{\\lambda_i}$\n",
    "\n",
    "Let's prove it! We know that $A^T A$ is symmetric and positive definite (meaning that $x^T A^T A x$ is strictly positive for non-zero vectors $x$), which implies that its eigenvalues are positive, and so we can always decompose it as:\n",
    "\n",
    "&emsp; $A^T A = V \\Lambda V^T$\n",
    "\n",
    "If we take the vectors $v_i$ of $V$ and multiply these vectors to $A$, we get something of that form:\n",
    "\n",
    "&emsp; $A v_i = \\sigma_i u_i$, where $||u_i|| = 1$\n",
    "\n",
    "Now we can prove that the vectors $u_i$ are orthonormal:\n",
    "\n",
    "&emsp; $(A v_j)^T A v_i = v_j^T A^T A v_i = \\lambda_i v_j^T v_i = \\lambda_i \\delta_{ij}$ (because $v_i$ is an eigen vector of $A^T A$)\n",
    "\n",
    "&emsp; $(A v_j)^T A v_i = \\sigma_j \\sigma_i u_j^T u_j \\implies u_j^T u_i = \\delta_{ij}$ and $\\lambda_i = \\sigma_i^2$\n",
    "\n",
    "And so, by bulking the products for all the eigen vectors, we get:\n",
    "\n",
    "&emsp; $A V = U \\Sigma \\implies A = U \\Sigma V^T$ (because $V$ is orthnormal)\n",
    "\n",
    "Finally, we can prove that $U$ contains eigen vectors of $A A^T$ by multiplying the whole:\n",
    "\n",
    "&emsp; $A A^T = U \\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T$\n",
    "\n",
    "&emsp; $A A^T U = U \\Lambda'$ where $\\Lambda' = \\Sigma \\Sigma^T$ is diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### SVD Example:\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\implies A^T = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$\n",
    "\n",
    "We compute the product of $A^T A$ to get a symmetric matrix and do an eigen decomposition:\n",
    "\n",
    "&emsp; $A^T A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$ has two eigenvalues $\\lambda_0 = 1$ and $\\lambda_1 = 3$ with eigenvectors $v_0 = (\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}})$ and $v_1 = (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}})$\n",
    "\n",
    "We can check that $v_0$ and $v_1$ are normalized and orthogonal to each other, and then build our first two matrices:\n",
    "\n",
    "&emsp; $V = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\implies V^T = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$\n",
    "\n",
    "Now we multiply the column vectors of $V$ to $A$ to get the vectors of $U$:\n",
    "\n",
    "&emsp; $\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = u_0$\n",
    "\n",
    "&emsp; $\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{2}{\\sqrt{2}} \\end{pmatrix} = \\sqrt{3} \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\\\ \\frac{\\sqrt{2}}{\\sqrt{3}} \\end{pmatrix} = \\sqrt{3} u_1$\n",
    "\n",
    "We therefore have the decomposition (which is quite useful):\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sqrt{3} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$\n",
    "\n",
    "To complete the SVD decomposition, we need to make $U$ and $V$ square orthonormal matrices, completing with orthogonal vectors if necessary:\n",
    "\n",
    "&emsp; $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} \\\\ 0 & \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sqrt{3} \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.08248290e-01  7.07106781e-01]\n",
      " [-4.08248290e-01 -7.07106781e-01]\n",
      " [-8.16496581e-01 -1.22629285e-16]]\n",
      "[[1.73205081 0.        ]\n",
      " [0.         1.        ]]\n",
      "[[-0.70710678 -0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "[[ 1.00000000e+00 -1.65911125e-16]\n",
      " [ 6.22328532e-19  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00]]\n",
      "[[-4.08248290e-01  7.07106781e-01 -5.77350269e-01]\n",
      " [-4.08248290e-01 -7.07106781e-01 -5.77350269e-01]\n",
      " [-8.16496581e-01 -1.22629285e-16  5.77350269e-01]]\n",
      "[[1.73205081 0.        ]\n",
      " [0.         1.        ]]\n",
      "[[-0.70710678 -0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "u, s, vt = np.linalg.svd(A, compute_uv=True, full_matrices=False)\n",
    "print(u)\n",
    "print(np.diag(s))\n",
    "print(vt)\n",
    "print(u @ np.diag(s) @ vt)\n",
    "\n",
    "u, s, vt = np.linalg.svd(A, compute_uv=True, full_matrices=True)\n",
    "print(u)\n",
    "print(np.diag(s))\n",
    "print(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
