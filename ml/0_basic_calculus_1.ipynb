{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Differentials, Gradients, Level Curves, Hessian, Jacobians\n",
    "---\n",
    "\n",
    "Lots of examples below are given for a function of 2 variables, but are easily extended for any number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Partial derivatives\n",
    "\n",
    "We define the partial derivative of a function $f(x,y)$ with respect to $x$ as the limit of the slope of change of $f$ when we modify $x$ by $dx$:\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial f}{\\partial x} = \\underset{h \\rightarrow \\infty}{lim} \\frac{f(x+h,y)}{h}$\n",
    "&emsp; and\n",
    "&emsp; $\\displaystyle \\frac{\\partial f}{\\partial y} = \\underset{h \\rightarrow \\infty}{lim} \\frac{f(x,y+h)}{h}$\n",
    "\n",
    "Intuitively, we can think of it as \"small change of $f$ over small change of $x$\", but this intuition is also really confusing when it comes to thinking \"how small is the change\". It helps to keep in mind that the formal definition is about limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Differentials\n",
    "\n",
    "We define the differential of $f(x)$, as **the linear function that best approximate** $f(x+dx) - f(x)$ in the neighborhood of $x$:\n",
    "\n",
    "&emsp; $\\boxed{df = df(x,dx) = \\sum \\frac{\\partial f}{\\partial x_i} dx_i = dx^T \\nabla f(x) = \\langle \\nabla f(x), dx \\rangle}$\n",
    "&emsp; where\n",
    "&emsp; $\\nabla f$ is the gradient defined after\n",
    "\n",
    "Directions $dx$ along which the differential is null define a plan whose equation is $df(x,dx) = 0$. This is the plan along which variations of $f$ are null. For instance, for a function of two arguments $f(x,y)$, the equation of the plan is:\n",
    "\n",
    "&emsp; $\\displaystyle df = \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy = 0$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $df = 0$ defines a plan whose normal is $\\displaystyle \\big (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\big )$\n",
    "\n",
    "Similarly, $f(x) + df(x, dx)$ actually describes the **equation of a plan tangent to $f$ at the point $x$**:\n",
    "\n",
    "&emsp; $\\displaystyle df = \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\displaystyle \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy + f(x, y) = z$\n",
    "&emsp; which is a plan whose normal is\n",
    "&emsp; $\\displaystyle \\big (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, -1 \\big )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Gradient\n",
    "\n",
    "Consider the function $f(x,y)$, we define the gradient of $f$ as:\n",
    "\n",
    "&emsp; $\\boxed{ \\nabla f = \\big ( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\big ) }$\n",
    "&emsp; which is a vector holding the **partial derivatives of $f$** with respect to each of its inputs variables\n",
    "\n",
    "The gradient as several interesting properties, which offer different point of views on it:\n",
    "\n",
    "1. We have $df = \\nabla f . dx \\implies $ the change of $f$ along direction $u$ is $\\nabla f . u$\n",
    "2. The gradient is **orthogonal to the level curves of $f$** (level are curves are such that $df=0$)\n",
    "3. The gradient gives the **direction along which the value of $f$ changes the fastest** (thee steepest ascent)\n",
    "\n",
    "Property (3) can be seen from property (1): the dot product between two vectors $u$ and $v$ is equal to $\\Vert u \\Vert \\Vert v \\Vert \\cos \\theta$, where $\\theta$ is the angle formed in the plan spanned by both vectors $u$ and $v$. The dot product and is therefore maximized when $\\theta = 0$, that is when the two vectors are colinear and pointing in the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Jacobian\n",
    "\n",
    "Say the function $f$ has $N$ inputs and $M$ outputs. We can see it as a collection of functions $f_1$ to $f_M$, each of which produces a component (the value of one dimension). The Jacobian is defined as a matrix where **each row contains the gradient of one of the component**:\n",
    "\n",
    "&emsp; $\\displaystyle J = \\begin{pmatrix} \\nabla f_1 \\\\ \\vdots \\\\ \\nabla f_M \\end{pmatrix}$\n",
    "&emsp; or \n",
    "&emsp; $\\displaystyle J = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_N} \\\\ \\vdots & & \\vdots \\\\ \\frac{\\partial f_M}{\\partial x_1} & \\dots & \\frac{\\partial f_M}{\\partial x_N} \\end{pmatrix}$\n",
    "\n",
    "To compute the differentials along each dimensions, and similarly was what we did for the gradient, we multiply the Jacobian matrix with a vector $dx = (dx_1, \\dots dx_N)^T$ containing the differentials of the inputs:\n",
    "\n",
    "&emsp; $\\displaystyle \\begin{pmatrix} df_1 \\\\ \\vdots \\\\ df_M \\end{pmatrix} = \\begin{pmatrix} \\nabla f_1 . dx \\\\ \\vdots \\\\ \\nabla f_M . dx \\end{pmatrix} = \\begin{pmatrix} \\nabla f_1 \\\\ \\vdots \\\\ \\nabla f_M \\end{pmatrix} \\begin{pmatrix} dx_1 \\\\ \\vdots \\\\ dx_N \\end{pmatrix} = J_f \\; dx$\n",
    "\n",
    "Note that the Jacobian has properties that are similar to the gradient: the level curves are replaced by the **null space (or kernel)** of the Jacobian: all the vector $u$ such that: $J u = 0$. This equation is similar to the **orthogonality property** of the gradient. The Jacobian of a function with just one output is the transpose of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Hessian matrix\n",
    "\n",
    "The Hessian matrix is the Jacobian of the gradient of $f$. Indeed, the gradient can be seen as a function with multiple components: the partial derivatives of $f$ along each of its input dimensions. The Hessian matrix is therefore equal to:\n",
    "\n",
    "&emsp; $\\displaystyle H = \\nabla \\nabla f = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1 \\partial x_1} & \\dots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_N} \\\\ \\vdots & & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_N \\partial x_1} & \\dots & \\frac{\\partial^2 f}{\\partial x_N \\partial x_N} \\end{pmatrix}$\n",
    "&emsp; which is a symmetric matrix since $\\displaystyle \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}$\n",
    "\n",
    "The \"definite-ness\" of the Hessian matrix allows to identify whether a critical point $\\nabla f = 0$ is:\n",
    "\n",
    "* a local minimum: $H$ is positive definite, i.e. $x^T H x \\ge 0, \\forall x$\n",
    "* a local maximum: $H$ is negative definite, i.e. $x^T H x \\le 0, \\forall x$\n",
    "* a saddle point otherwise\n",
    "\n",
    "Since the Hessian is symmetric, it has real eigenvalues, and we can evaluate its \"definite-ness\" by looking at the sign of these eigenvalues. If they are all positive, the matrix is positive definite. If they are all negative, the matrix is negative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Chain rule\n",
    "\n",
    "If we consider a function $f(x,y)$, where $x(u,v)$ and $y(u,v)$ are themselves functions, we can compute the variations of $f$ with respect to $u$ and $v$ by using the chain rule:\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{\\partial f}{\\partial u} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial u} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial u}$\n",
    "&emsp; and\n",
    "&emsp; $\\displaystyle \\frac{\\partial f}{\\partial v} = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial v} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial v}$\n",
    "\n",
    "The chain rule follows from the definition of differentials, if we just replace $dx$ and $dy$ by their definition and keep the terms of the component we are interested about, for instance $du$ if we are interested in the partial derivative with respect to $u$:\n",
    "\n",
    "&emsp; $\\displaystyle df = \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy$\n",
    "&emsp; with \n",
    "&emsp; $\\displaystyle dx = \\frac{\\partial x}{\\partial u} du + \\frac{\\partial x}{\\partial v} dv$\n",
    "&emsp; and \n",
    "&emsp; $\\displaystyle dy = \\frac{\\partial y}{\\partial u} du + \\frac{\\partial y}{\\partial v} dv$\n",
    "\n",
    "The chain rule can be written more generically as:\n",
    "\n",
    "&emsp; $\\boxed{\\frac{\\partial f}{\\partial y_j} = \\sum_i \\frac{\\partial f}{\\partial x_i} \\frac{\\partial x_i}{\\partial y_j}}$\n",
    "&emsp; where $x_i$ are the input variable of $f$ that depends on $y_j$\n",
    "\n",
    "We see from the indices that it ressembles matrix multiplication. It can indeed be re-written as a **product between the gradient of $f$ with respect to $x$ and the Jacobian of $x$ with respect to $y$**:\n",
    "\n",
    "&emsp; $\\displaystyle (\\nabla_{y} f)^T = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} & \\dots & \\frac{\\partial f}{\\partial x_M} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x_1}{\\partial y_1} & \\dots & \\frac{\\partial x_1}{\\partial y_N} \\\\ \\vdots & & \\vdots \\\\ \\frac{\\partial x_M}{\\partial y_1} & \\dots & \\frac{\\partial x_M}{\\partial y_N} \\end{pmatrix}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\boxed{(\\nabla_{y} f)^T = (\\nabla_x f)^T J_y(x)}$\n",
    "&emsp; or\n",
    "&emsp; $\\boxed{J_{y}(f) = J_x(f) J_y(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Product and quotient rule\n",
    "\n",
    "As specific application of the chain rule, we can derive the **product rule**:\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{d \\big( u(x) v(x)\\big)}{dx} = \\frac{\\partial \\big(u(x) v(x) \\big)}{\\partial u} \\frac{\\partial u}{\\partial x} + \\frac{\\partial \\big(u(x) v(x) \\big)}{\\partial v} \\frac{\\partial v}{\\partial x}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\boxed{(uv)' = u'v + v'u}$\n",
    "\n",
    "Similarly, we can derive the **quotient rule**:\n",
    "\n",
    "&emsp; $\\displaystyle \\frac{d}{dx} \\Big( \\frac{u(x)}{v(x)} \\Big) = \\frac{\\partial}{\\partial u} \\Big( \\frac{u(x)}{v(x)} \\Big) \\frac{\\partial u}{\\partial x} + \\frac{\\partial}{\\partial v} \\Big( \\frac{u(x)}{v(x)} \\Big) \\frac{\\partial v}{\\partial x}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\boxed{\\Big(\\frac{u}{v} \\Big)' = \\frac{u' v - v' u}{v^2}}$\n",
    "\n",
    "These rules are applicable for multivariable functions as well:\n",
    "\n",
    "&emsp; $\\displaystyle \\forall i, \\frac{\\partial u(x) v(x)}{\\partial x_i} = \\frac{\\partial u(x)}{\\partial x_i} v(x) + u(x) \\frac{\\partial v(x)}{\\partial x_i}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $\\boxed{\\nabla (uv) = \\nabla u \\times v + u \\times \\nabla v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Taylors expansion\n",
    "\n",
    "The Taylor expansion for a **single variable function** $f(x)$ in the neighborhood of $a$:\n",
    "\n",
    "&emsp; $\\displaystyle f(x) = \\sum_{n=0}^{\\infty} {\\frac{f^{(n)}(a)}{n!}}(x-a)^n$\n",
    "&emsp; where\n",
    "&emsp; $f^{(n)}$ is the $n^{th}$ derivative of $f$\n",
    "\n",
    "**Proof:** Recursively match each order, by first matching the value of the function, then the value of the derivative, then the value of the second derivative, and so on... The factorials naturally appears as the result of differentiating a polynomial:\n",
    "\n",
    "&emsp; $\\displaystyle f^{(0)}(x) = \\sum_{n=0}^{\\infty} b_n (x-a)^n$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $f(a) = b_0$\n",
    "\n",
    "&emsp; $\\displaystyle f^{(1)}(x) = \\sum_{n=1}^{\\infty} n b_n (x-a)^{n-1}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $f^{(1)}(a) = 1 \\times b_1$\n",
    "\n",
    "&emsp; $\\displaystyle f^{(2)}(x) = \\sum_{n=2}^{\\infty} n (n-1) b_n (x-a)^{n-2}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $f^{(2)}(a) = 2 \\times 1 \\times b_2$\n",
    "\n",
    "&emsp; $\\displaystyle f^{(3)}(x) = \\sum_{n=3}^{\\infty} n (n-1) (n-2) b_n (x-a)^{n-3}$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $f^{(3)}(a) = 3 \\times 2 \\times 1 \\times b_3$\n",
    "\n",
    "&emsp; $\\dots$\n",
    "\n",
    "<br>\n",
    "\n",
    "### Taylors expansion (multivariate)\n",
    "\n",
    "The Taylor expansion for a **multi variable function** $f(x)$ in the neighborhood of $a$:\n",
    "\n",
    "&emsp; $\\displaystyle f(x) = \\sum_{|\\alpha| = 0}^{\\infty} {\\frac {D^\\alpha f(a)}{|\\alpha|!}}(x-a)^{\\alpha}$\n",
    "&emsp; where\n",
    "&emsp; $|\\alpha| = \\sum_n \\alpha_i$\n",
    "&emsp; and\n",
    "&emsp; $D^{\\alpha} f = \\frac {\\partial^{|\\alpha|} f}{\\partial x_1^{\\alpha_1} \\cdots \\partial x_n^{\\alpha_n}}$\n",
    "\n",
    "The Taylor expansion at order 2 is better known in terms of the gradient and the Hessian matrix:\n",
    "\n",
    "&emsp; $\\displaystyle f(x) \\simeq f(a) + (x - a)^T \\nabla f + (x - a)^T H (x-a)$\n",
    "\n",
    "This formula also helps to understand why we need to check the \"definite-ness\" of the Hessian in order to check if a critical point is a local minimum, a local maximum, or a saddle point. Indeed, if the gradient is null, the Hessian determines in which direction $f(x)$ will move in the neighborhood of $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Integrals\n",
    "---\n",
    "\n",
    "* fundamental theorem of calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
