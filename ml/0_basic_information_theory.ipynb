{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - entropy - definition\n",
    "# TODO - mutual information\n",
    "# TODO - KL divergence\n",
    "# TODO - compress some data using neural net - compare to max compression\n",
    "# TODO - arithmetic coding, huffman code, LZ coding, limit of shannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Information theory\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Information in an event\n",
    "\n",
    "The amount of information of event x ocurring with probability p is:\n",
    "\n",
    "&emsp; $h(x) = \\log \\frac{1}{p(x)} = - \\log p(x)$\n",
    "\n",
    "The logarithm satisfies some interesting properties such as the amount of information of two independent events, with probability $p(x)p(y)$, is the sum of the information of both events. Also, the information of an event that is 100% probable is none (we learn nothing about the world), while the information of an impossible event would be infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The entropy of a **discrete probability distribution** is the expected value of the amount of information:\n",
    "\n",
    "&emsp; $H[X] = E_{x \\sim X}[h(x)] = E_{x \\sim X}[\\log p(x)] = - \\sum p(x) \\log p(x)$\n",
    "\n",
    "The conditional entropy is the expected value of the conditional probability:\n",
    "\n",
    "&emsp; $H[X|Y] = - \\sum_{X,Y} p(x,y) \\log p(x|y) = - \\sum_{Y} p(y) \\sum_{X} p(x|y) \\log p(x|y)$\n",
    "\n",
    "And we have the equivalent of Bayes' rule:\n",
    "\n",
    "&emsp; $H[X,Y] = H[X|Y] + H[Y] = H[Y|X] + H[X]$\n",
    "\n",
    "The entropy is **not defined for probability densities** (check [this link](https://en.wikipedia.org/wiki/Entropy_(information_theory)#Extending_discrete_entropy_to_the_continuous_case))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### KL divergence & Relative entropy\n",
    "\n",
    "The relative entropy of two sources of information is the KL divergence:\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(p||q) = - \\sum p(x) \\log \\frac{q(x)}{p(x)}$\n",
    "&emsp; where\n",
    "&emsp; $q(x) = 0 \\implies p(x) = 0$\n",
    "&emsp; (absolute continuity property)\n",
    "\n",
    "The relative entropy is **valid for probability densities**:\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(p||q) = - \\int p(x) \\log \\frac{q(x)}{p(x)} dx$\n",
    "\n",
    "This KL diverge is:\n",
    "\n",
    "* always positive and equal to zero only when $p$ is equal to $q$\n",
    "* not symmetric $D_{KL}(p||q) \\ne D_{KL}(q||p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Mutual information\n",
    "\n",
    "The mutual information between two random variables $X$ and $Y$ (where $X$ is traditionally the source and $Y$ is the traditionnally the destination), is defined has the reduction of uncertainty (that is the reduction of information) of $X$ knowing the value of $Y$:\n",
    "\n",
    "&emsp; $I(X;Y) = H[Y] - H[Y|X] = H[X] - H[X|Y] = I(Y;X)$\n",
    "\n",
    "This quantity is related to the KL divergence:\n",
    "\n",
    "&emsp; $I(X;Y) = D_{KL}(p(X,Y)||p(X)p(Y)) = E_Y[D_{KL}(p(X|Y)||p(X))]$\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "&emsp; $I(X;Y) = H[X] - H[X|Y] = - \\sum_{X} p(x) \\log p(x) + \\sum_{X,Y} p(x,y) \\log p(x|y)$\n",
    "\n",
    "&emsp; $I(X;Y) = - \\sum_{X,Y} p(x,y) \\log p(x) + \\sum_{X,Y} p(x,y) \\log p(x|y) = \\sum_{X,Y} p(x,y) \\log \\frac{p(x|y)}{p(x)} = \\sum_{X,Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# KL divergence and Density Estimation\n",
    "---\n",
    "\n",
    "Theoritically, minimizing the KL divergence between two probability distributions $p$ and $q$ will make sure that both distributions ends up being equal. But when $p$ and $q$ are constrained to different models, in general, this will not be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Asymmetry when minimizing KL divergence\n",
    "\n",
    "Say there is an unknown **data generating process** $p$ that you would like to approximate with a **model $q$** of a constrained form, an activity which we call **density estimation**. Minimizing the KL divergence will lead to different results depending which side you take.\n",
    "\n",
    "Minimizing the KL divergence of our model $q$ with the distribution $q$ will tend to **underestimate the variance**:\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(q||p) = - \\int q(x) \\log \\frac{p(x)}{q(x)} dx$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $q(x)$ will need to be equal to zero if $p(x)$ is zero ($p$ will include $q$)\n",
    "\n",
    "Minimizing the KL diverenge of the distribution $p$ with our model $q$ will tend to **overestimate the variance**:\n",
    "\n",
    "&emsp; $\\displaystyle D_{KL}(p||q) = - \\int p(x) \\log \\frac{p(x)}{q(x)} dx$\n",
    "&emsp; $\\implies$\n",
    "&emsp; $p(x)$ will need to be equal to zero if $q(x)$ is zero ($q$ will include $p$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Example\n",
    "\n",
    "**todo** try to approximate a mixture of 2 gaussian with just 1 gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
